{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    notebook = \"NN/TabNet\"\n",
    "    script = \"nn/tabnet\"\n",
    "\n",
    "    n_splits = 5\n",
    "    seed = 2020\n",
    "    target = \"target\"\n",
    "\n",
    "    batch_size = 512\n",
    "    max_epochs = 60\n",
    "\n",
    "    # Colab Env\n",
    "    api_path = \"/content/drive/MyDrive/workspace/kaggle.json\"\n",
    "    drive_path = \"/content/drive/MyDrive/workspace/kaggle-amex\"\n",
    "\n",
    "    # Kaggle Env\n",
    "    kaggle_dataset_path = None\n",
    "\n",
    "    # Reka Env\n",
    "    dir_path = '/home/abe/kaggle/kaggle-amex'\n",
    "\n",
    "    def is_notebook():\n",
    "        if 'get_ipython' not in globals():\n",
    "            return False\n",
    "        env_name = get_ipython().__class__.__name__  # type: ignore\n",
    "        if env_name == 'TerminalInteractiveShell':\n",
    "            return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import shutil\n",
    "import logging\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "import gc\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import pickle\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "import ntpath\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-pastel')\n",
    "sns.set_palette(\"winter_r\")\n",
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(Config.seed)\n",
    "np.random.seed(Config.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(Config.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = os.path.join(Config.dir_path, 'input')\n",
    "OUTPUT = os.path.join(Config.dir_path, 'output')\n",
    "SUBMISSION = os.path.join(Config.dir_path, 'submissions')\n",
    "OUTPUT_EXP = os.path.join(OUTPUT, Config.script)\n",
    "EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n",
    "EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n",
    "NOTEBOOK = os.path.join(Config.dir_path, \"Notebooks\")\n",
    "SCRIPT = os.path.join(Config.dir_path, \"scripts\")\n",
    "\n",
    "# make dirs\n",
    "for dir in [\n",
    "        INPUT,\n",
    "        OUTPUT,\n",
    "        SUBMISSION,\n",
    "        OUTPUT_EXP,\n",
    "        EXP_MODEL,\n",
    "        EXP_FIG,\n",
    "        NOTEBOOK,\n",
    "        SCRIPT]:\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "if Config.is_notebook():\n",
    "    notebook_path = os.path.join(NOTEBOOK, Config.notebook + \".ipynb\")\n",
    "    script_path = os.path.join(SCRIPT, Config.script + \".py\")\n",
    "    dir, _ = ntpath.split(script_path)\n",
    "    subprocess.run(f\"mkdir -p {dir}; touch {script_path}\",\n",
    "                   shell=True,\n",
    "                   stdout=PIPE,\n",
    "                   stderr=PIPE,\n",
    "                   text=True)\n",
    "    subprocess.run(\n",
    "        f\"jupyter nbconvert --to python {notebook_path} --output {script_path}\",\n",
    "        shell=True,\n",
    "        stdout=PIPE,\n",
    "        stderr=PIPE,\n",
    "        text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(INPUT, 'train.parquet'))\n",
    "target = pd.read_csv(\n",
    "    os.path.join(\n",
    "        INPUT,\n",
    "        'train_labels.csv'),\n",
    "    dtype={\n",
    "        'customer_ID': 'str',\n",
    "        'target': 'int8'})\n",
    "# train = pd.read_parquet(os.path.join(INPUT, 'train_small.parquet') if COLAB else 'train_small.parquet')\n",
    "test = pd.read_parquet(os.path.join(INPUT, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amex metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "class AmexTabnet(Metric):\n",
    "    \n",
    "  def __init__(self):\n",
    "    self._name = 'amex_tabnet'\n",
    "    self._maximize = True\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    amex = amex_metric(y_true, y_pred[:, 1])\n",
    "    return max(amex, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "features_avg = ['B_11', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n",
    "                'B_20', 'B_28', 'B_29', 'B_3', 'B_33', 'B_36', 'B_37', 'B_4', 'B_42', \n",
    "                'B_5', 'B_8', 'B_9', 'D_102', 'D_103', 'D_105', 'D_111', 'D_112', 'D_113', \n",
    "                'D_115', 'D_118', 'D_119', 'D_121', 'D_124', 'D_128', 'D_129', 'D_131', \n",
    "                'D_132', 'D_133', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', 'D_145', \n",
    "                'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', \n",
    "                'D_49', 'D_50', 'D_51', 'D_52', 'D_56', 'D_58', 'D_62', 'D_70', 'D_71', \n",
    "                'D_72', 'D_74', 'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_91', \n",
    "                'P_2', 'P_3', 'R_1', 'R_10', 'R_11', 'R_13', 'R_18', 'R_19', 'R_2', 'R_26', \n",
    "                'R_27', 'R_28', 'R_3', 'S_11', 'S_12', 'S_22', 'S_23', 'S_24', 'S_26', \n",
    "                'S_27', 'S_5', 'S_7', 'S_8', ]\n",
    "features_min = ['B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_19', 'B_2', 'B_20', 'B_22', \n",
    "                'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_33', 'B_36', 'B_4', 'B_42', \n",
    "                'B_5', 'B_9', 'D_102', 'D_103', 'D_107', 'D_109', 'D_110', 'D_111', \n",
    "                'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', \n",
    "                'D_129', 'D_132', 'D_133', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', \n",
    "                'D_145', 'D_39', 'D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', \n",
    "                'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', \n",
    "                'D_71', 'D_74', 'D_75', 'D_78', 'D_79', 'D_81', 'D_83', 'D_84', 'D_86', \n",
    "                'D_88', 'D_96', 'P_2', 'P_3', 'P_4', 'R_1', 'R_11', 'R_13', 'R_17', 'R_19', \n",
    "                'R_2', 'R_27', 'R_28', 'R_4', 'R_5', 'R_8', 'S_11', 'S_12', 'S_23', 'S_25', \n",
    "                'S_3', 'S_5', 'S_7', 'S_9', ]\n",
    "features_max = ['B_1', 'B_11', 'B_13', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n",
    "                'B_22', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_31', 'B_33', 'B_36', \n",
    "                'B_4', 'B_42', 'B_5', 'B_7', 'B_9', 'D_102', 'D_103', 'D_105', 'D_109', \n",
    "                'D_110', 'D_112', 'D_113', 'D_115', 'D_121', 'D_124', 'D_128', 'D_129', \n",
    "                'D_131', 'D_139', 'D_141', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', \n",
    "                'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_52', \n",
    "                'D_53', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_72', 'D_74', \n",
    "                'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_89', 'P_2', 'P_3', \n",
    "                'R_1', 'R_10', 'R_11', 'R_26', 'R_28', 'R_3', 'R_4', 'R_5', 'R_7', 'R_8', \n",
    "                'S_11', 'S_12', 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', ]\n",
    "features_last = ['B_1', 'B_11', 'B_12', 'B_13', 'B_14', 'B_16', 'B_18', 'B_19', 'B_2', \n",
    "                 'B_20', 'B_21', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30', 'B_31', \n",
    "                 'B_33', 'B_36', 'B_37', 'B_38', 'B_39', 'B_4', 'B_40', 'B_42', 'B_5', \n",
    "                 'B_8', 'B_9', 'D_102', 'D_105', 'D_106', 'D_107', 'D_108', 'D_110', \n",
    "                 'D_111', 'D_112', 'D_113', 'D_114', 'D_115', 'D_116', 'D_117', 'D_118', \n",
    "                 'D_119', 'D_120', 'D_121', 'D_124', 'D_126', 'D_128', 'D_129', 'D_131', \n",
    "                 'D_132', 'D_133', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', \n",
    "                 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', \n",
    "                 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_55', \n",
    "                 'D_56', 'D_59', 'D_60', 'D_62', 'D_63', 'D_64', 'D_66', 'D_68', 'D_70', \n",
    "                 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_77', 'D_78', 'D_81', 'D_82', \n",
    "                 'D_83', 'D_84', 'D_88', 'D_89', 'D_91', 'D_94', 'D_96', 'P_2', 'P_3', \n",
    "                 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_16', 'R_17', 'R_18', \n",
    "                 'R_19', 'R_25', 'R_28', 'R_3', 'R_4', 'R_5', 'R_8', 'S_11', 'S_12', \n",
    "                 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', 'S_9', ]\n",
    "features_categorical = ['B_30_last', 'B_38_last', 'D_114_last', 'D_116_last',\n",
    "                        'D_117_last', 'D_120_last', 'D_126_last',\n",
    "                        'D_63_last', 'D_64_last', 'D_66_last', 'D_68_last']\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse=False, dtype=np.float32, handle_unknown='ignore')\n",
    "\n",
    "def add_features(df, is_train):\n",
    "    cid = pd.Categorical(df.pop('customer_ID'), ordered=True)\n",
    "    last = (cid != np.roll(cid, -1))\n",
    "    df_avg = (df\n",
    "              .groupby(cid)\n",
    "              .mean()[features_avg]\n",
    "              .rename(columns={f: f\"{f}_avg\" for f in features_avg})\n",
    "             )\n",
    "    gc.collect()\n",
    "    df_min = (df\n",
    "              .groupby(cid)\n",
    "              .min()[features_min]\n",
    "              .rename(columns={f: f\"{f}_min\" for f in features_min})\n",
    "             )\n",
    "    gc.collect()\n",
    "    df_max = (df\n",
    "              .groupby(cid)\n",
    "              .max()[features_max]\n",
    "              .rename(columns={f: f\"{f}_max\" for f in features_max})\n",
    "             )\n",
    "    gc.collect()\n",
    "    df_last = (df.loc[last, features_last]\n",
    "          .rename(columns={f: f\"{f}_last\" for f in features_last})\n",
    "          .set_index(np.asarray(cid[last]))\n",
    "         )\n",
    "    gc.collect()\n",
    "    \n",
    "    df_categorical = df_last[features_categorical].astype(object)\n",
    "    features_not_cat = [f for f in df_last.columns if f not in features_categorical]\n",
    "    if is_train:\n",
    "        encoder.fit(df_categorical)\n",
    "    df_categorical = pd.DataFrame(encoder.transform(df_categorical).astype(np.float16),\n",
    "                                  index=df_categorical.index).rename(columns=str)\n",
    "    \n",
    "    df = pd.concat([df_last[features_not_cat], df_categorical, df_avg, df_min, df_max], axis=1)\n",
    "    df.fillna(value=0, inplace=True)\n",
    "    \n",
    "    \n",
    "    del df_avg, df_max, df_min, df_last, df_categorical, cid, last, features_not_cat\n",
    "    return df\n",
    "\n",
    "train = add_features(train, True)\n",
    "test = add_features(test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(target.set_index('customer_ID'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "unuse = ['customer_ID', 'S_2', 'target']\n",
    "\n",
    "features = [col for col in train.columns if col not in unuse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def fit_tabnet(X, y):\n",
    "    models = []\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances[\"feature\"] = X.columns.tolist()\n",
    "    stats = pd.DataFrame()\n",
    "    explain_matrices = []\n",
    "    masks = []\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=Config.n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=Config.seed)\n",
    "\n",
    "    for fold, (train_indices, valid_indices) in enumerate(\n",
    "            skf.split(X, y)):\n",
    "        X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        X_valid, y_valid = X.iloc[valid_indices], y.iloc[valid_indices]\n",
    "        print('#'*25)\n",
    "        print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "        print('#'*25)\n",
    "\n",
    "        \n",
    "        model = TabNetClassifier(n_d = 32,\n",
    "                             n_a = 32,\n",
    "                             n_steps = 3,\n",
    "                             gamma = 1.3,\n",
    "                             n_independent = 2,\n",
    "                             n_shared = 2,\n",
    "                             momentum = 0.02,\n",
    "                             clip_value = None,\n",
    "                             lambda_sparse = 1e-3,\n",
    "                             optimizer_fn = torch.optim.Adam,\n",
    "                             optimizer_params = dict(lr = 1e-3, weight_decay=1e-3),\n",
    "                             scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                             scheduler_params = {'T_0':5,\n",
    "                                                 'eta_min':1e-4,\n",
    "                                                 'T_mult':1,\n",
    "                                                 'last_epoch':-1},\n",
    "                             mask_type = 'entmax',\n",
    "                             seed = Config.seed)\n",
    "        \n",
    "        model.fit(np.array(X_train),\n",
    "              np.array(y_train.values.ravel()),\n",
    "              eval_set = [(np.array(X_valid), np.array(y_valid.values.ravel()))],\n",
    "              max_epochs = Config.max_epochs,\n",
    "              patience = 50,\n",
    "              batch_size = Config.batch_size,\n",
    "              eval_metric = ['auc', 'accuracy', AmexTabnet])\n",
    "\n",
    "        # ------------------- prediction -------------------\n",
    "        pred = model.predict_proba(X_valid.values)[:,1]\n",
    "        score = amex_metric(y_valid, pred)\n",
    "\n",
    "\n",
    "        # Loss ,metric, improtances tracking\n",
    "        stats[f'fold{fold+1}_train_loss'] = model.history['loss']\n",
    "        stats[f'fold{fold+1}_val_metric'] = model.history['val_0_amex_tabnet']\n",
    "        feature_importances[f\"importance_fold{fold}+1\"] = model.feature_importances_\n",
    "        \n",
    "        # model explanability\n",
    "        explain_matrix, mask = model.explain(X_valid.values)\n",
    "        explain_matrices.append(explain_matrix)\n",
    "        masks.append(mask[0])\n",
    "        masks.append(mask[1])\n",
    "        \n",
    "        scores.append(score)\n",
    "        models.append(model)\n",
    "        print(f'fold{fold} amex meric: {score}')\n",
    "        print()\n",
    "\n",
    "    print(f\"OOF Score: {np.mean(scores):.5f}\")\n",
    "    return models, explain_matrix, masks, stats, feature_importances\n",
    "\n",
    "\n",
    "def inference_tabnet(models, X):\n",
    "    pred = np.array([model.predict_proba(X.values) for model in models])\n",
    "    pred = np.mean(pred, axis=0)[:, 1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, explain_matrix, masks, stats, feature_importances = fit_tabnet(train[features], train[Config.target])\n",
    "pred = inference_tabnet(models, test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stats.filter(like='train', axis=1).columns.tolist():\n",
    "    plt.plot(stats[i], label=str(i))\n",
    "plt.title('Train loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'{EXP_FIG}/loss.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stats.filter(like='val', axis=1).columns.tolist():\n",
    "    plt.plot(stats[i], label=str(i))\n",
    "plt.title('Train RMSPE')\n",
    "plt.legend()\n",
    "plt.savefig(f'{EXP_FIG}/rmspe.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['mean_importance']=feature_importances[['importance_fold0+1','importance_fold1+1']].mean(axis=1)\n",
    "feature_importances.sort_values(by='mean_importance', ascending=False, inplace=True)\n",
    "sns.barplot(y=feature_importances['feature'][:50],x=feature_importances['mean_importance'][:50])\n",
    "plt.title('Mean Feature Importance by Folds')\n",
    "plt.savefig(f'{EXP_FIG}/importance.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'customer_ID': test.index,\n",
    "                    'prediction': pred})\n",
    "sub.to_csv(f'{SUBMISSION}/tabnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('amex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29001d83105739b7e7991894c59ca9a685d5fb80e48bd2a6126a67de75b87e20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
