{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    name = \"EDA/Feature-RFE\"\n",
    "\n",
    "    n_splits = 5\n",
    "    seed = 2022\n",
    "    target = \"target\"\n",
    "\n",
    "    # Colab Env\n",
    "    upload_from_colab = True\n",
    "    api_path = \"/content/drive/MyDrive/workspace/kaggle.json\"\n",
    "    drive_path = \"/content/drive/MyDrive/workspace/kaggle-amex\"\n",
    "\n",
    "    # Kaggle Env\n",
    "    kaggle_dataset_path = None\n",
    "\n",
    "    # Reka Env\n",
    "    dir_path = '/home/abe/kaggle/kaggle-amex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import shutil\n",
    "import logging\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "import gc\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = os.path.join(Config.dir_path, 'input')\n",
    "OUTPUT = os.path.join(Config.dir_path, 'output')\n",
    "SUBMISSION = os.path.join(Config.dir_path, 'submissions')\n",
    "OUTPUT_EXP = os.path.join(OUTPUT, Config.name)\n",
    "EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n",
    "EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n",
    "EXP_PREDS = os.path.join(OUTPUT_EXP, \"preds\")\n",
    "\n",
    "# make dirs\n",
    "for d in [INPUT, SUBMISSION, EXP_MODEL, EXP_FIG, EXP_PREDS]:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(INPUT, 'train.parquet'))\n",
    "test = pd.read_parquet(os.path.join(INPUT, 'test.parquet'))\n",
    "target = pd.read_csv(os.path.join(INPUT, 'train_labels.csv'), dtype={'customer_ID': 'str', 'target': 'int8'})\n",
    "\n",
    "with open(os.path.join(INPUT, 'data_types.json'), \"r\") as rf:\n",
    "    dtypes = json.load(rf)\n",
    "train = train.astype(dtypes)\n",
    "test = test.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5531451 entries, 0 to 5531450\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: category(2), datetime64[ns](1), float16(185), int8(1), object(1)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_136</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>0.938477</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>1.006836</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.126709</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009575</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.009216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-05-28</td>\n",
       "      <td>0.954102</td>\n",
       "      <td>0.091492</td>\n",
       "      <td>0.021652</td>\n",
       "      <td>1.009766</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.123962</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.002604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>0.960449</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>1.002930</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.009598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>0.947266</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>0.009827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID        S_2       P_2  \\\n",
       "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-03-09  0.938477   \n",
       "1  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-04-07  0.936523   \n",
       "2  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-05-28  0.954102   \n",
       "3  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-06-13  0.960449   \n",
       "4  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-07-16  0.947266   \n",
       "\n",
       "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  ...  \\\n",
       "0  0.001734  0.008728  1.006836  0.009224  0.124023  0.008774  0.004707  ...   \n",
       "1  0.005775  0.004925  1.000977  0.006153  0.126709  0.000798  0.002714  ...   \n",
       "2  0.091492  0.021652  1.009766  0.006817  0.123962  0.007599  0.009422  ...   \n",
       "3  0.002455  0.013687  1.002930  0.001372  0.117188  0.000685  0.005531  ...   \n",
       "4  0.002483  0.015190  1.000977  0.007607  0.117310  0.004654  0.009308  ...   \n",
       "\n",
       "   D_136  D_137  D_138     D_139     D_140     D_141  D_142     D_143  \\\n",
       "0    NaN    NaN    NaN  0.002426  0.003706  0.003819    NaN  0.000569   \n",
       "1    NaN    NaN    NaN  0.003956  0.003166  0.005032    NaN  0.009575   \n",
       "2    NaN    NaN    NaN  0.003269  0.007328  0.000427    NaN  0.003429   \n",
       "3    NaN    NaN    NaN  0.006119  0.004517  0.003201    NaN  0.008423   \n",
       "4    NaN    NaN    NaN  0.003672  0.004944  0.008888    NaN  0.001670   \n",
       "\n",
       "      D_144     D_145  \n",
       "0  0.000610  0.002674  \n",
       "1  0.005493  0.009216  \n",
       "2  0.006985  0.002604  \n",
       "3  0.006527  0.009598  \n",
       "4  0.008125  0.009827  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wwBWY1fdnxFw"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/inversion/amex-competition-metric-python\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def lgb_amex_metric(y_true, y_pred):\n",
    "    \"\"\"The competition metric with lightgbm's calling convention\"\"\"\n",
    "    return ('amex',\n",
    "            amex_metric(pd.DataFrame({'target': y_true}), pd.Series(y_pred, name='prediction')),\n",
    "            True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [col for col in train.columns if train[col].dtype == 'category']\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[col])\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.groupby('customer_ID')\\\n",
    "  .tail(1).set_index('customer_ID', drop=True)\\\n",
    "  .sort_index()\\\n",
    "  .merge(target.set_index('customer_ID', drop=True), left_index=True, right_index=True)\n",
    "test =  test.groupby('customer_ID')\\\n",
    "  .tail(1).set_index('customer_ID', drop=True)\\\n",
    "  .sort_index()\n",
    "train.reset_index(drop=False, inplace=True)\n",
    "test.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Features to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "unuse = ['target', 'customer_ID', 'S_2']\n",
    "\n",
    "for col in train.columns:\n",
    "  if col not in unuse:\n",
    "    features.append(col)\n",
    "\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[features].values, train[Config.target].values,\n",
    "                 train_size=0.8, \n",
    "                 random_state=Config.seed, \n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 188 features.\n",
      "Fitting estimator with 187 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 185 features.\n",
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 181 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 179 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 176 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 172 features.\n",
      "Fitting estimator with 171 features.\n",
      "Fitting estimator with 170 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "lgb_params = {\"learning_rate\": 0.01,\n",
    "              'num_leaves': 127,\n",
    "              'min_child_samples': 2400}\n",
    "\n",
    "fit_params = {\n",
    "    'callbacks': [early_stopping(stopping_rounds=10, verbose=0)],\n",
    "    'eval_set': [(X_test, y_test)],\n",
    "    'eval_metric': lgb_amex_metric,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(**lgb_params,\n",
    "                       boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       n_estimators=10000,\n",
    "                       random_state=Config.seed,\n",
    "                       force_col_wise=True,\n",
    "                       n_jobs=32,\n",
    "                       verbose=-1)\n",
    "\n",
    "rfe = RFE(model,\n",
    "          n_features_to_select=100,\n",
    "          step=1,\n",
    "          verbose=1)\n",
    "\n",
    "rfe.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.DataFrame(rfe.transform(train[features]), \n",
    "                     columns=train[features].columns.values[rfe.get_support()])\n",
    "result = pd.DataFrame(rfe.get_support(), index=train[features].columns.values, columns=['used'])\n",
    "result['ranking'] = rfe.ranking_\n",
    "result.sort_values('ranking', ascending=True).reset_index(drop=False, inplace=True)\n",
    "result.to_csv(f'{EXP_MODEL}/rfe_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.plotting import plot_metric\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def fit_lgbm(X, y, params=None):\n",
    "  models = []\n",
    "  scores = []\n",
    "\n",
    "  skf = StratifiedKFold(n_splits=Config.n_splits, shuffle=True, random_state=Config.seed)\n",
    "  \n",
    "  for fold, (train_indices, valid_indices) in enumerate(tqdm(skf.split(X, y))):\n",
    "    print(\"-\"*50+f' fold{fold} '+'-'*50)\n",
    "    X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "    X_valid, y_valid = X.iloc[valid_indices], y.iloc[valid_indices]\n",
    "\n",
    "    model = LGBMClassifier(**params,\n",
    "                           boosting_type='gbdt',\n",
    "                           objective='binary',\n",
    "                           n_estimators=10000,\n",
    "                           random_state=Config.seed,\n",
    "                           force_col_wise=True,\n",
    "                           n_jobs=32,\n",
    "                           verbose=-1)\n",
    "    \n",
    "    model.fit(X_train, y_train, \n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_names=['train', 'valid'],\n",
    "              eval_metric=lgb_amex_metric,\n",
    "              callbacks=[early_stopping(stopping_rounds=10, verbose=0)],\n",
    "              verbose=50)\n",
    "    \n",
    "    # ------------------- prediction -------------------\n",
    "    pred = model.predict_proba(X_valid)[:, 1]\n",
    "    score = amex_metric(pd.DataFrame({'target': y_valid.values}), pd.Series(pred, name='prediction'))\n",
    "\n",
    "    # ------------------- plot -------------------\n",
    "    plot_metric(model)\n",
    "\n",
    "    # ------------------- save -------------------\n",
    "    file = f'{EXP_MODEL}/lgbm_fold{fold}.pkl'\n",
    "    joblib.dump(model, file)\n",
    "    scores.append(score)\n",
    "    models.append(model)\n",
    "    print(f'fold{fold} amex meric: {score}')\n",
    "    print()\n",
    "\n",
    "  print(f\"OOF Score: {np.mean(scores):.5f}\")\n",
    "  return models\n",
    "\n",
    "def inference_lgbm(models, X):\n",
    "    pred = np.array([model.predict_proba(X) for model in models])\n",
    "    pred = np.mean(pred, axis=0)[:, 1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\"learning_rate\": 0.01,\n",
    "              'num_leaves': 127,\n",
    "              'min_child_samples': 2400}\n",
    "\n",
    "features = list(train_new.columns)\n",
    "models = fit_lgbm(train[features], train[Config.target], params=lgb_params)\n",
    "# models = [joblib.load(f'{EXP_MODEL}/lgbm_fold{i}.pkl') for i in range(Config.n_splits)]\n",
    "pred = inference_lgbm(models, test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importances(models):\n",
    "    importance_df = pd.DataFrame(models[0].feature_importances_, \n",
    "                                 index=features, \n",
    "                                 columns=['importance'])\\\n",
    "                        .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    plt.subplots(figsize=(len(features) // 4, 5))\n",
    "    plt.bar(importance_df.index, importance_df.importance)\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_importances(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'customer_ID': test.index,\n",
    "                    'prediction': pred})\n",
    "sub.to_csv(f'{EXP_PREDS}/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle competitions submit -c amex-default-prediction -f /home/abe/kaggle/kaggle-amex/submissions/submission.csv -m \"Recuresive Feature Elimination for Baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29001d83105739b7e7991894c59ca9a685d5fb80e48bd2a6126a67de75b87e20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('amex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
